import numpy as np

def f(x1, x2):
    return 7*x1**2 + 2*x1*x2 + 5*x2**2 + x1 - 10*x2

def grad_f(x1, x2):
    grad_x1 = 14*x1 + 2*x2 + 1
    grad_x2 = 2*x1 + 10*x2 - 10
    return np.array([grad_x1, grad_x2])

def gradient_descent_optimization(x0, learning_rate, max_iterations, stopping_criteria):
    x = x0
    for i in range(max_iterations):
        gradient = grad_f(x[0], x[1])
        x_new = x - learning_rate * gradient
        if np.linalg.norm(x_new - x) < stopping_criteria:
            break
        x = x_new
    return x

# Начальное приближение
x0 = np.array([0, 0])

# Шаг спуска
learning_rate = 0.1

# Точность остановки
stopping_criteria = 1e-6

# Максимальное количество итераций
max_iterations = 1000

# Выполняем метод градиентного спуска
optimum = gradient_descent_optimization(x0, learning_rate, max_iterations, stopping_criteria)

# Выводим результат
print("Оптимум найден в точке (x1, x2) = ({:.4f}, {:.4f})".format(optimum[0], optimum[1]))
print("Значение функции в оптимуме: {:.4f}".format(f(optimum[0], optimum[1])))
